name: Data Engineering CI

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  ci:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U airflow"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          # Using 3.10 ensures we get the latest stable 3.10.x patches
          python-version: "3.10"
          cache: 'pip'

      - name: Install dependencies
        env:
          SKIP_ARROW_BUILD: 1
        run: |
          # 1. Prepare environment
          python -m pip install --upgrade pip setuptools wheel

          # 2. Stage 1: Force-install the "Conflict Makers" as binaries.
          # This sets a 'floor' so pip cannot backtrack to old source versions.
          pip install --only-binary :all: numpy==1.26.4 pyarrow==14.0.2 dill==0.3.8

          # 3. Define the official Airflow constraints for this Python version
          export CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.10.txt"

          # 4. Stage 2: Unified Install
          # We include the dev tools (pytest/ruff/black) here so they don't 
          # cause a secondary conflict later.
          pip install --only-binary :all: \
            --constraint "${CONSTRAINT_URL}" \
            -r requirements.txt \
            pytest==8.0.0 ruff==0.1.9 black==24.1.1

          # 5. Local project install
          pip install -e .

      - name: Lint and Format
        run: |
          ruff check .
          black --check .

      - name: Run unit tests
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: python -m pytest

      - name: Validate Airflow DAGs
        env:
          PYTHONPATH: ${{ github.workspace }}
          AIRFLOW__CORE__EXECUTOR: SequentialExecutor
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__LOAD_EXAMPLES: "false"
        run: |
          airflow db migrate
          python - <<EOF
          import os
          from airflow.models import DagBag
          dag_folder = os.path.join(os.getcwd(), 'airflow/dags')
          dagbag = DagBag(dag_folder=dag_folder, include_examples=False)
          if dagbag.import_errors:
              for filename, errors in dagbag.import_errors.items():
                  print(f"File: {filename}\nErrors: {errors}")
              exit(1)
          print(f"Loaded {len(dagbag.dags)} DAGs successfully")
          EOF